{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Importing the Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T04:53:33.767415Z","iopub.status.busy":"2024-12-04T04:53:33.767181Z","iopub.status.idle":"2024-12-04T04:53:43.099575Z","shell.execute_reply":"2024-12-04T04:53:43.098420Z","shell.execute_reply.started":"2024-12-04T04:53:33.767395Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["! pip install transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:01:28.616328Z","iopub.status.busy":"2024-12-04T05:01:28.615616Z","iopub.status.idle":"2024-12-04T05:01:38.271225Z","shell.execute_reply":"2024-12-04T05:01:38.270375Z","shell.execute_reply.started":"2024-12-04T05:01:28.616294Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n","  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n","/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n","caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n","  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","import os\n","import json\n","import random\n","from PIL import Image\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{},"source":["# Defining Data Locations"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:01:47.466800Z","iopub.status.busy":"2024-12-04T05:01:47.465492Z","iopub.status.idle":"2024-12-04T05:01:47.471720Z","shell.execute_reply":"2024-12-04T05:01:47.470923Z","shell.execute_reply.started":"2024-12-04T05:01:47.466758Z"},"trusted":true},"outputs":[],"source":["trainval_image_dir = os.path.join('/kaggle/input/coco-image-caption', 'train2014', 'train2014')\n","trainval_captions_dir = os.path.join('/kaggle/input/coco-image-caption', 'annotations_trainval2014', 'annotations')\n","test_image_dir = os.path.join('/kaggle/input/coco-image-caption', 'val2017', 'val2017')\n","test_captions_dir = os.path.join('/kaggle/input/coco-image-caption', 'annotations_trainval2017', 'annotations')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:01:48.569426Z","iopub.status.busy":"2024-12-04T05:01:48.568742Z","iopub.status.idle":"2024-12-04T05:01:48.573475Z","shell.execute_reply":"2024-12-04T05:01:48.572608Z","shell.execute_reply.started":"2024-12-04T05:01:48.569396Z"},"trusted":true},"outputs":[],"source":["trainval_captions_filepath = os.path.join(trainval_captions_dir, 'captions_train2014.json')\n","test_captions_filepath = os.path.join(test_captions_dir, 'captions_val2017.json')"]},{"cell_type":"markdown","metadata":{},"source":["# Splitting Data into Train and Validation Set\n","\n","We'll be using 20% of train_2014 data to be used as our Validation Set and rest as Training Set"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:01:50.332546Z","iopub.status.busy":"2024-12-04T05:01:50.331958Z","iopub.status.idle":"2024-12-04T05:01:56.087206Z","shell.execute_reply":"2024-12-04T05:01:56.086350Z","shell.execute_reply.started":"2024-12-04T05:01:50.332516Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset size: 66226\n","Valid dataset size: 16557\n"]}],"source":["all_filepaths = np.array([os.path.join(trainval_image_dir, f) for f in os.listdir(trainval_image_dir)])\n","rand_indices = np.arange(len(all_filepaths))\n","np.random.shuffle(rand_indices)\n","\n","split = int(len(all_filepaths)*0.8)\n","\n","train_filepaths, valid_filepaths = all_filepaths[rand_indices[:split]], all_filepaths[rand_indices[split:]] \n","\n","print(f\"Train dataset size: {len(train_filepaths)}\")\n","print(f\"Valid dataset size: {len(valid_filepaths)}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Processing Data\n","\n","Here we'll be making train, valid and test dataframes"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:02:16.502643Z","iopub.status.busy":"2024-12-04T05:02:16.502305Z","iopub.status.idle":"2024-12-04T05:02:20.704493Z","shell.execute_reply":"2024-12-04T05:02:20.703554Z","shell.execute_reply.started":"2024-12-04T05:02:16.502603Z"},"trusted":true},"outputs":[],"source":["with open(trainval_captions_filepath, 'r') as f:\n","    trainval_data = json.load(f)\n","    \n","trainval_captions_df = pd.json_normalize(trainval_data, \"annotations\")\n","trainval_captions_df[\"image_filepath\"] = trainval_captions_df[\"image_id\"].apply(\n","    lambda x: os.path.join(trainval_image_dir, 'COCO_train2014_'+format(x, '012d')+'.jpg')\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:03:04.088646Z","iopub.status.busy":"2024-12-04T05:03:04.088012Z","iopub.status.idle":"2024-12-04T05:03:05.074762Z","shell.execute_reply":"2024-12-04T05:03:05.074076Z","shell.execute_reply.started":"2024-12-04T05:03:04.088609Z"},"trusted":true},"outputs":[],"source":["def preprocess_captions(image_captions_df):\n","    \"\"\" Preprocessing the captions \"\"\"\n","    \n","    image_captions_df[\"preprocessed_caption\"] = \"[START] \" + image_captions_df[\"caption\"].str.lower().str.replace('[^\\w\\s]','') + \" [END]\"\n","    return image_captions_df\n","\n","train_captions_df = trainval_captions_df[trainval_captions_df[\"image_filepath\"].isin(train_filepaths)]\n","train_captions_df = preprocess_captions(train_captions_df)\n","valid_captions_df = trainval_captions_df[trainval_captions_df[\"image_filepath\"].isin(valid_filepaths)]\n","valid_captions_df = preprocess_captions(valid_captions_df)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:03:06.738640Z","iopub.status.busy":"2024-12-04T05:03:06.738372Z","iopub.status.idle":"2024-12-04T05:03:08.888852Z","shell.execute_reply":"2024-12-04T05:03:08.888006Z","shell.execute_reply.started":"2024-12-04T05:03:06.738618Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>788.000000</td>\n","      <td>788.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>567.157360</td>\n","      <td>498.809645</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>97.566102</td>\n","      <td>99.834981</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>331.000000</td>\n","      <td>182.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>480.000000</td>\n","      <td>427.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>640.000000</td>\n","      <td>480.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>640.000000</td>\n","      <td>640.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>640.000000</td>\n","      <td>640.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                0           1\n","count  788.000000  788.000000\n","mean   567.157360  498.809645\n","std     97.566102   99.834981\n","min    331.000000  182.000000\n","25%    480.000000  427.000000\n","50%    640.000000  480.000000\n","75%    640.000000  640.000000\n","max    640.000000  640.000000"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["n_samples = 1000\n","\n","train_image_stats_df = train_captions_df.loc[:n_samples, \"image_filepath\"].apply(lambda x: Image.open(x).size)\n","train_image_stats_df = pd.DataFrame(train_image_stats_df.tolist(), index=train_image_stats_df.index)\n","train_image_stats_df.describe()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:03:08.890041Z","iopub.status.busy":"2024-12-04T05:03:08.889771Z","iopub.status.idle":"2024-12-04T05:03:10.602882Z","shell.execute_reply":"2024-12-04T05:03:10.601825Z","shell.execute_reply.started":"2024-12-04T05:03:08.890008Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["3638\n"]}],"source":["train_vocabulary = train_captions_df[\"preprocessed_caption\"].str.split(\" \").explode().value_counts()\n","print(len(train_vocabulary[train_vocabulary>=25]))"]},{"cell_type":"markdown","metadata":{},"source":["# Understanding the Bert Tokenizer"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:03:10.605492Z","iopub.status.busy":"2024-12-04T05:03:10.605119Z","iopub.status.idle":"2024-12-04T05:03:15.525742Z","shell.execute_reply":"2024-12-04T05:03:15.524375Z","shell.execute_reply.started":"2024-12-04T05:03:10.605461Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"source":["from tokenizers import BertWordPieceTokenizer\n","\n","# Initialize an empty BERT tokenizer\n","tokenizer = BertWordPieceTokenizer(\n","    #reserved_tokens=[\"[UNK]\", \"[START]\", \"[END]\", \"[PAD]\"],\n","    unk_token=\"[UNK]\",\n","    #trainer_params=None,\n","    #vocab_size=8000,\n","    clean_text=False,\n","    lowercase=False,\n",")\n","\n","tokenizer.train_from_iterator(\n","    train_captions_df[\"preprocessed_caption\"].tolist(),\n","    vocab_size=4000,\n","    special_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:03:15.526990Z","iopub.status.busy":"2024-12-04T05:03:15.526776Z","iopub.status.idle":"2024-12-04T05:03:15.534489Z","shell.execute_reply":"2024-12-04T05:03:15.533482Z","shell.execute_reply.started":"2024-12-04T05:03:15.526972Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[START] a very clean and well decorated empty bathroom [END] -> ['[START]', 'a', 'very', 'clean', 'and', 'well', 'decorated', 'empty', 'bathroom', '[END]']\n","[START] a blue and white bathroom with butterfly themed wall tiles [END] -> ['[START]', 'a', 'blue', 'and', 'white', 'bathroom', 'with', 'butter', '##fl', '##y', 'them', '##ed', 'wall', 'tiles', '[END]']\n","[START] a bathroom with a border of butterflies and blue paint on the walls above it [END] -> ['[START]', 'a', 'bathroom', 'with', 'a', 'bor', '##der', 'of', 'butter', '##fl', '##ies', 'and', 'blue', 'paint', 'on', 'the', 'walls', 'above', 'it', '[END]']\n","[START] an angled view of a beautifully decorated bathroom [END] -> ['[START]', 'an', 'ang', '##led', 'view', 'of', 'a', 'beautiful', '##ly', 'decorated', 'bathroom', '[END]']\n","[START] the vanity contains two sinks with a towel for each [END] -> ['[START]', 'the', 'vanity', 'contains', 'two', 'sinks', 'with', 'a', 'towel', 'for', 'each', '[END]']\n","[START] green tiled backsplash highlighted by low overhead lighting [END] -> ['[START]', 'green', 'tiled', 'backs', '##pla', '##sh', 'high', '##light', '##ed', 'by', 'low', 'overhead', 'lighting', '[END]']\n","[START] a purple bus and a man dressed as a nun on a tall bicycle [END] -> ['[START]', 'a', 'purple', 'bus', 'and', 'a', 'man', 'dressed', 'as', 'a', 'n', '##un', 'on', 'a', 'tall', 'bicycle', '[END]']\n","[START] clean bathroom that is brightly colored and has a window [END] -> ['[START]', 'clean', 'bathroom', 'that', 'is', 'brightly', 'colored', 'and', 'has', 'a', 'window', '[END]']\n","[START] a black cat making an angry face while sitting on the bathroom floor  [END] -> ['[START]', 'a', 'black', 'cat', 'making', 'an', 'ang', '##ry', 'face', 'while', 'sitting', 'on', 'the', 'bathroom', 'floor', '[END]']\n","[START] a kitchen counter is illuminated by a hood light [END] -> ['[START]', 'a', 'kitchen', 'counter', 'is', 'illuminated', 'by', 'a', 'hood', 'light', '[END]']\n"]}],"source":["# Encoding a sentence\n","example_captions = valid_captions_df[\"preprocessed_caption\"].iloc[:10].tolist()\n","example_tokenized_captions = tokenizer.encode_batch(example_captions)\n","\n","for caption, tokenized_cap in zip(example_captions, example_tokenized_captions):\n","    print(f\"{caption} -> {tokenized_cap.tokens}\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:03:15.536405Z","iopub.status.busy":"2024-12-04T05:03:15.535466Z","iopub.status.idle":"2024-12-04T05:03:15.545232Z","shell.execute_reply":"2024-12-04T05:03:15.544278Z","shell.execute_reply.started":"2024-12-04T05:03:15.536382Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[UNK] -> 1\n","[PAD] -> 0\n","[START] -> 2\n","[END] -> 3\n"]}],"source":["vocab = tokenizer.get_vocab()\n","\n","for token in [\"[UNK]\", \"[PAD]\", \"[START]\", \"[END]\"]:\n","    print(f\"{token} -> {vocab[token]}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Defining the `tf.data.Dataset` for image captioning"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:03:15.546527Z","iopub.status.busy":"2024-12-04T05:03:15.546289Z","iopub.status.idle":"2024-12-04T05:03:15.556227Z","shell.execute_reply":"2024-12-04T05:03:15.555370Z","shell.execute_reply.started":"2024-12-04T05:03:15.546508Z"},"trusted":true},"outputs":[],"source":["def parse_image(filepath, resize_height, resize_width):\n","    image = tf.io.read_file(filepath)\n","    image = tf.io.decode_jpeg(image, channels=3)\n","    image = tf.image.convert_image_dtype(image, tf.float32)\n","    image = tf.image.resize(image, [resize_height, resize_width])\n","    image = image*2.0 - 1.0\n","    return image"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:03:15.557555Z","iopub.status.busy":"2024-12-04T05:03:15.557269Z","iopub.status.idle":"2024-12-04T05:03:15.572697Z","shell.execute_reply":"2024-12-04T05:03:15.571916Z","shell.execute_reply.started":"2024-12-04T05:03:15.557529Z"},"trusted":true},"outputs":[],"source":["def generate_tokenizer(captions_df, n_vocab):\n","    \"\"\" Generate the tokenizer with given captions \"\"\"\n","    \n","    # Define the tokenizer\n","    tokenizer = BertWordPieceTokenizer(\n","        unk_token=\"[UNK]\",\n","        clean_text=False,\n","        lowercase=False,\n","    )\n","    \n","    # Train the tokenizer\n","    tokenizer.train_from_iterator(\n","        captions_df[\"preprocessed_caption\"].tolist(),\n","        vocab_size=n_vocab,\n","        special_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","    )\n","    \n","    return tokenizer"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:03:15.574862Z","iopub.status.busy":"2024-12-04T05:03:15.574615Z","iopub.status.idle":"2024-12-04T05:03:15.585178Z","shell.execute_reply":"2024-12-04T05:03:15.584383Z","shell.execute_reply.started":"2024-12-04T05:03:15.574839Z"},"trusted":true},"outputs":[],"source":["def generate_tf_dataset(image_captions_df, tokenizer=None, n_vocab=5000, pad_length=33, batch_size=32, training=False):\n","    \"\"\" Generate the tf.data.Dataset\"\"\"\n","    \n","    # If the tokenizer is not available, create one\n","    if not tokenizer:\n","        tokenizer = generate_tokenizer(image_captions_df, n_vocab)\n","        \n","    # Get the caption IDs using the tokenizer\n","    image_captions_df[\"caption_token_ids\"] = [enc.ids for enc in tokenizer.encode_batch(image_captions_df[\"preprocessed_caption\"])]\n","    \n","    vocab = tokenizer.get_vocab()\n","    \n","    # Add the padding to short sentences and truncate long ones\n","    image_captions_df[\"caption_token_ids\"] = image_captions_df[\"caption_token_ids\"].apply(\n","        lambda x: x+[vocab[\"[PAD]\"]]*(pad_length - len(x) + 2) if pad_length + 2 >= len(x) else x[:pad_length + 1] + [x[-1]]\n","    ) \n","    \n","    # Create a dataset with images and captions\n","    dataset = tf.data.Dataset.from_tensor_slices({\n","        \"image_filepath\": image_captions_df[\"image_filepath\"],\n","        \"caption_token_ids\": np.array(image_captions_df[\"caption_token_ids\"].tolist())\n","    })\n","    \n","    # Each sample in our dataset consists of\n","    # (image, caption token IDs, position IDs), (caption token IDs offset by 1)\n","    dataset = dataset.map(\n","        lambda x: (\n","            (parse_image(x[\"image_filepath\"], 224, 224), x[\"caption_token_ids\"][:-1], tf.range(pad_length+1, dtype='float32')), x[\"caption_token_ids\"]\n","        )\n","    )\n","    \n","    # Shuffle and batch data in the training mode\n","    if training:\n","        dataset = dataset.shuffle(buffer_size=batch_size*10)\n","    \n","    dataset = dataset.batch(batch_size)\n","    \n","    return dataset, tokenizer"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:03:15.586598Z","iopub.status.busy":"2024-12-04T05:03:15.586288Z","iopub.status.idle":"2024-12-04T05:03:29.878761Z","shell.execute_reply":"2024-12-04T05:03:29.877735Z","shell.execute_reply.started":"2024-12-04T05:03:15.586570Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","((<tf.Tensor: shape=(2, 224, 224, 3), dtype=float32, numpy=\n","array([[[[-0.34959733, -0.3629464 , -0.43932068],\n","         [-0.25675768, -0.26680666, -0.36203605],\n","         [-0.17223382, -0.18939072, -0.26907384],\n","         ...,\n","         [-0.96056587, -0.98127663, -1.        ],\n","         [-0.96172976, -0.98244053, -0.99662995],\n","         [-0.9795432 , -0.9838324 , -0.9957108 ]],\n","\n","        [[-0.47532386, -0.47450978, -0.5141281 ],\n","         [-0.4415266 , -0.4537815 , -0.5031074 ],\n","         [-0.41738445, -0.4453256 , -0.47346812],\n","         ...,\n","         [-0.9663871 , -0.97423023, -1.        ],\n","         [-0.97191876, -0.97760844, -1.        ],\n","         [-0.9797976 , -0.9797976 , -0.99476564]],\n","\n","        [[-0.454508  , -0.45916492, -0.5023546 ],\n","         [-0.43404233, -0.44775033, -0.48939073],\n","         [-0.4263655 , -0.44254202, -0.47463232],\n","         ...,\n","         [-0.94553614, -0.9732322 , -1.        ],\n","         [-0.9525386 , -0.98391116, -1.        ],\n","         [-0.96531916, -0.9789656 , -0.99465185]],\n","\n","        ...,\n","\n","        [[-0.6791229 , -0.8227941 , -0.93468136],\n","         [-0.7129989 , -0.85955006, -0.9718225 ],\n","         [-0.7070903 , -0.7810399 , -0.877521  ],\n","         ...,\n","         [-0.51228935, -0.5432238 , -0.7941781 ],\n","         [-0.5030376 , -0.526567  , -0.7697042 ],\n","         [-0.54204243, -0.56557184, -0.80953246]],\n","\n","        [[-0.72535884, -0.87662816, -0.95475316],\n","         [-0.7125437 , -0.831285  , -0.9491159 ],\n","         [-0.72777486, -0.8318715 , -0.92658436],\n","         ...,\n","         [-0.5020742 , -0.53301823, -0.7926642 ],\n","         [-0.53508395, -0.5406688 , -0.79218304],\n","         [-0.52823937, -0.5596632 , -0.7984941 ]],\n","\n","        [[-0.7519958 , -0.9082983 , -0.9628939 ],\n","         [-0.7383841 , -0.8420255 , -0.96247375],\n","         [-0.73807776, -0.85908616, -0.95367646],\n","         ...,\n","         [-0.51521325, -0.5443455 , -0.8003676 ],\n","         [-0.55742276, -0.55294114, -0.8022408 ],\n","         [-0.5365038 , -0.5723561 , -0.8087713 ]]],\n","\n","\n","       [[[-1.        , -1.        , -1.        ],\n","         [-1.        , -1.        , -1.        ],\n","         [-1.        , -1.        , -1.        ],\n","         ...,\n","         [-1.        , -1.        , -1.        ],\n","         [-1.        , -1.        , -1.        ],\n","         [-1.        , -1.        , -1.        ]],\n","\n","        [[-1.        , -1.        , -1.        ],\n","         [-1.        , -1.        , -1.        ],\n","         [-1.        , -1.        , -1.        ],\n","         ...,\n","         [-1.        , -1.        , -1.        ],\n","         [-1.        , -1.        , -1.        ],\n","         [-1.        , -1.        , -1.        ]],\n","\n","        [[-1.        , -1.        , -1.        ],\n","         [-1.        , -1.        , -1.        ],\n","         [-1.        , -1.        , -1.        ],\n","         ...,\n","         [-1.        , -1.        , -1.        ],\n","         [-1.        , -1.        , -1.        ],\n","         [-1.        , -1.        , -1.        ]],\n","\n","        ...,\n","\n","        [[-0.99583334, -0.99583334, -0.99583334],\n","         [-0.99583334, -0.99583334, -0.99583334],\n","         [-0.99583334, -0.99583334, -0.99583334],\n","         ...,\n","         [-0.99583334, -0.99583334, -0.99583334],\n","         [-0.99583334, -0.99583334, -0.99583334],\n","         [-0.99583334, -0.99583334, -0.99583334]],\n","\n","        [[-0.90955883, -0.90955883, -0.90955883],\n","         [-0.90955883, -0.90955883, -0.90955883],\n","         [-0.90955883, -0.90955883, -0.90955883],\n","         ...,\n","         [-0.90955883, -0.90955883, -0.90955883],\n","         [-0.90955883, -0.90955883, -0.90955883],\n","         [-0.90955883, -0.90955883, -0.90955883]],\n","\n","        [[-0.6784314 , -0.6784314 , -0.6784314 ],\n","         [-0.6784314 , -0.6784314 , -0.6784314 ],\n","         [-0.6784314 , -0.6784314 , -0.6784314 ],\n","         ...,\n","         [-0.6784314 , -0.6784314 , -0.6784314 ],\n","         [-0.6784314 , -0.6784314 , -0.6784314 ],\n","         [-0.6784314 , -0.6784314 , -0.6784314 ]]]], dtype=float32)>, <tf.Tensor: shape=(2, 11), dtype=int64, numpy=\n","array([[   2,  159,  657, 1075,  335,  108,   23,  343,  467,  162,  105],\n","       [   2,   23, 1104,  129, 2702,  482,  106,   23,  343,  115, 1247]])>, <tf.Tensor: shape=(2, 11), dtype=float32, numpy=\n","array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n","       [ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]],\n","      dtype=float32)>), <tf.Tensor: shape=(2, 12), dtype=int64, numpy=\n","array([[   2,  159,  657, 1075,  335,  108,   23,  343,  467,  162,  105,\n","           3],\n","       [   2,   23, 1104,  129, 2702,  482,  106,   23,  343,  115, 1247,\n","           3]])>)\n"]}],"source":["n_vocab=4000\n","batch_size=2\n","sample_dataset, sample_tokenizer = generate_tf_dataset(train_captions_df, n_vocab=n_vocab, pad_length=10, batch_size=batch_size, training=True)\n","for i in sample_dataset.take(1):\n","    print(i)"]},{"cell_type":"markdown","metadata":{},"source":["# Defining the model\n","\n","Our model consists of,\n","\n","* A Vision Transformer(ViT) - Takes in patches of images as inputs and produce a sequence of output representations for each patch\n","* A Text Decoder Transformer - Takes in the final representation of the vision Transformer, along with input caption IDs and predict the next token in the caption for each time step\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:08:48.555669Z","iopub.status.busy":"2024-12-04T05:08:48.554950Z","iopub.status.idle":"2024-12-04T05:08:55.569820Z","shell.execute_reply":"2024-12-04T05:08:55.568907Z","shell.execute_reply.started":"2024-12-04T05:08:48.555639Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Final representation shape: (None, 384)\n"]}],"source":["import tensorflow_hub as hub\n","import tensorflow.keras.backend as K\n","\n","K.clear_session()\n","\n","image_input = tf.keras.layers.Input(shape=(224, 224, 3))\n","image_encoder = hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_s16_fe/1\", trainable=False)\n","image_features = image_encoder(image_input)\n","print(f\"Final representation shape: {image_features.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["## The Text Decoder Transformer\n","\n","Here we define the text decoder. It takes the final image representation of ViT and concatenate that with caption IDs. Then we predict caption token ID from the next time step with the decoder."]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:08:55.571381Z","iopub.status.busy":"2024-12-04T05:08:55.571103Z","iopub.status.idle":"2024-12-04T05:09:02.740579Z","shell.execute_reply":"2024-12-04T05:09:02.739949Z","shell.execute_reply.started":"2024-12-04T05:08:55.571358Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n","                                )]                                                                \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," input_3 (InputLayer)           [(None, None)]       0           []                               \n","                                                                                                  \n"," keras_layer (KerasLayer)       (None, 384)          21665664    ['input_1[0][0]']                \n","                                                                                                  \n"," embedding (Embedding)          (None, None, 384)    1536000     ['input_2[0][0]']                \n","                                                                                                  \n"," lambda (Lambda)                (None, None, 384)    0           ['input_3[0][0]']                \n","                                                                                                  \n"," tf.expand_dims (TFOpLambda)    (None, 1, 384)       0           ['keras_layer[0][0]']            \n","                                                                                                  \n"," tf.__operators__.add (TFOpLamb  (None, None, 384)   0           ['embedding[0][0]',              \n"," da)                                                              'lambda[0][0]']                 \n","                                                                                                  \n"," concatenate (Concatenate)      (None, None, 384)    0           ['tf.expand_dims[0][0]',         \n","                                                                  'tf.__operators__.add[0][0]']   \n","                                                                                                  \n"," transformer_decoder_layer (Tra  (None, None, 384)   838016      ['concatenate[0][0]']            \n"," nsformerDecoderLayer)                                                                            \n","                                                                                                  \n"," transformer_decoder_layer_1 (T  (None, None, 384)   838016      ['transformer_decoder_layer[0][0]\n"," ransformerDecoderLayer)                                         ']                               \n","                                                                                                  \n"," transformer_decoder_layer_2 (T  (None, None, 384)   838016      ['transformer_decoder_layer_1[0][\n"," ransformerDecoderLayer)                                         0]']                             \n","                                                                                                  \n"," transformer_decoder_layer_3 (T  (None, None, 384)   838016      ['transformer_decoder_layer_2[0][\n"," ransformerDecoderLayer)                                         0]']                             \n","                                                                                                  \n"," dense_8 (Dense)                (None, None, 4000)   1540000     ['transformer_decoder_layer_3[0][\n","                                                                 0]']                             \n","                                                                                                  \n","==================================================================================================\n","Total params: 28,093,728\n","Trainable params: 6,428,064\n","Non-trainable params: 21,665,664\n","__________________________________________________________________________________________________\n"]}],"source":["class SelfAttentionLayer(tf.keras.layers.Layer):\n","    \"\"\" Defines the computations in the self attention layer \"\"\"\n","    \n","    def __init__(self, d):        \n","        super(SelfAttentionLayer, self).__init__()\n","        # Feature dimensionality of the output\n","        self.d = d\n","    \n","    def build(self, input_shape):\n","        # Query weight matrix\n","        self.Wq = self.add_weight(\n","            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n","            trainable=True, dtype='float32'\n","        )        \n","        # Key weight matrix\n","        self.Wk = self.add_weight(\n","            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n","            trainable=True, dtype='float32'\n","        )\n","        # Value weight matrix\n","        self.Wv = self.add_weight(\n","            shape=(input_shape[-1], self.d), initializer='glorot_uniform',\n","            trainable=True, dtype='float32'\n","        )\n","    \n","    def call(self, q_x, k_x, v_x, mask=None):\n","        \n","        q = tf.matmul(q_x,self.Wq) #[None, t, d]\n","        k = tf.matmul(k_x,self.Wk) #[None, t, d]\n","        v = tf.matmul(v_x,self.Wv) #[None, t, d]\n","        \n","        # Computing the final output\n","        h = tf.keras.layers.Attention(causal=True)([\n","            q, #q\n","            v, #v\n","            k, #k\n","        ], mask=[None, mask]) # [None, t, t] . [None, t, d] => [None, t, d]\n","        \n","        return h\n","    \n","    \n","class TransformerDecoderLayer(tf.keras.layers.Layer):\n","    \"\"\" The Decoder layer \"\"\"\n","    \n","    def __init__(self, d, n_heads):\n","        super(TransformerDecoderLayer, self).__init__()\n","        # Feature dimensionality\n","        self.d = d\n","        \n","        # Dimensionality of a head\n","        self.d_head = int(d/n_heads) \n","        \n","        # Number of heads\n","        self.n_heads = n_heads\n","        \n","        # Actual attention heads\n","        self.attn_heads = [SelfAttentionLayer(self.d_head) for i in range(self.n_heads)]\n","        \n","        # Fully connected layers\n","        self.fc1_layer = tf.keras.layers.Dense(512, activation='relu')\n","        self.fc2_layer = tf.keras.layers.Dense(d)\n","        \n","        self.add_layer = tf.keras.layers.Add()\n","        self.norm1_layer = tf.keras.layers.LayerNormalization()\n","        self.norm2_layer = tf.keras.layers.LayerNormalization()\n","        \n","    \n","    def _compute_multihead_output(self, x):\n","        \"\"\" Computing the multi head attention output\"\"\"\n","        outputs = [head(x, x, x) for head in self.attn_heads]            \n","        outputs = tf.concat(outputs, axis=-1)\n","        return outputs\n","        \n","    def call(self, x):\n","        \n","        \n","        # Multi head attention layer output\n","        h1 = self._compute_multihead_output(x)\n","        \n","        h1_add = self.add_layer([x, h1])\n","        h1_norm = self.norm1_layer(h1_add)\n","        \n","        # Fully connected outputs\n","        h2_1 = self.fc1_layer(h1_norm)\n","        h2_2 = self.fc2_layer(h2_1)\n","        \n","        h2_add = self.add_layer([h1, h2_2])\n","        h2_norm = self.norm2_layer(h2_add)\n","        \n","        \n","        return h2_norm\n","    \n","\n","# Input layer\n","caption_input = tf.keras.layers.Input(shape=(None,))\n","position_input = tf.keras.layers.Input(shape=(None,))\n","d_model = 384\n","\n","# Token embeddings\n","input_embedding = tf.keras.layers.Embedding(len(tokenizer.get_vocab()), d_model, mask_zero=True)\n","\n","# Position embeddings\n","position_embedding = tf.keras.layers.Lambda(\n","    lambda x: tf.where(\n","        tf.math.mod(tf.repeat(tf.expand_dims(x, axis=-1), d_model, axis=-1), 2)==0,\n","        tf.math.sin(\n","            #tf.repeat(tf.expand_dims(x, axis=-1), d_model, axis=-1) /\n","            tf.expand_dims(x, axis=-1) /\n","            10000**(2*tf.reshape(tf.range(d_model, dtype='float32'),[1,1, -1])/d_model)\n","        ),\n","        tf.math.cos(\n","            tf.expand_dims(x, axis=-1) /\n","            10000**(2*tf.reshape(tf.range(d_model, dtype='float32'),[1,1, -1])/d_model)\n","        )\n","    )\n",")\n","\n","# Combined token position embeddings\n","embed_out = input_embedding(caption_input) + position_embedding(position_input)\n","# Concatenate image caption and token embeddings\n","image_caption_embed_out = tf.keras.layers.Concatenate(axis=1)([tf.expand_dims(image_features,axis=1), embed_out])\n","\n","# Generate hidden representation with Transformer decoder layer\n","out = image_caption_embed_out\n","for l in range(4):\n","    out  = TransformerDecoderLayer(d_model, 64)(out)\n","\n","# Final prediction layer\n","final_out = tf.keras.layers.Dense(n_vocab, activation='softmax')(out)\n","\n","# Define the final model and compile\n","full_model = tf.keras.models.Model(inputs=[image_input, caption_input, position_input], outputs=final_out)\n","full_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics='accuracy')\n","\n","full_model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## Defining the Blue Metric"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:09:02.742256Z","iopub.status.busy":"2024-12-04T05:09:02.742008Z","iopub.status.idle":"2024-12-04T05:09:02.752371Z","shell.execute_reply":"2024-12-04T05:09:02.751527Z","shell.execute_reply.started":"2024-12-04T05:09:02.742234Z"},"trusted":true},"outputs":[],"source":["import collections\n","import math\n","\n","\n","def _get_ngrams(segment, max_order):\n","    \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n","\n","      Args:\n","        segment: text segment from which n-grams will be extracted.\n","        max_order: maximum length in tokens of the n-grams returned by this\n","            methods.\n","\n","      Returns:\n","        The Counter containing all n-grams upto max_order in segment\n","        with a count of how many times each n-gram occurred.\n","    \"\"\"\n","    ngram_counts = collections.Counter()\n","    for order in range(1, max_order + 1):\n","        for i in range(0, len(segment) - order + 1):\n","            ngram = tuple(segment[i:i+order])\n","            ngram_counts[ngram] += 1\n","    return ngram_counts\n","\n","\n","def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n","                 smooth=False):\n","    \"\"\"Computes BLEU score of translated segments against one or more references.\n","\n","      Args:\n","        reference_corpus: list of lists of references for each translation. Each\n","            reference should be tokenized into a list of tokens.\n","        translation_corpus: list of translations to score. Each translation\n","            should be tokenized into a list of tokens.\n","        max_order: Maximum n-gram order to use when computing BLEU score.\n","        smooth: Whether or not to apply Lin et al. 2004 smoothing.\n","\n","      Returns:\n","        3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n","        precisions and brevity penalty.\n","    \"\"\"\n","    matches_by_order = [0] * max_order\n","    possible_matches_by_order = [0] * max_order\n","    reference_length = 0\n","    translation_length = 0\n","    for (references, translation) in zip(reference_corpus,\n","                                           translation_corpus):\n","        reference_length += min(len(r) for r in references)\n","        translation_length += len(translation)\n","\n","        merged_ref_ngram_counts = collections.Counter()\n","        for reference in references:\n","            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n","        translation_ngram_counts = _get_ngrams(translation, max_order)\n","        overlap = translation_ngram_counts & merged_ref_ngram_counts\n","        for ngram in overlap:\n","            matches_by_order[len(ngram)-1] += overlap[ngram]\n","        for order in range(1, max_order+1):\n","            possible_matches = len(translation) - order + 1\n","            if possible_matches > 0:\n","                possible_matches_by_order[order-1] += possible_matches\n","\n","        precisions = [0] * max_order\n","        for i in range(0, max_order):\n","            if smooth:\n","                   precisions[i] = ((matches_by_order[i] + 1.) /\n","                           (possible_matches_by_order[i] + 1.))\n","            else:\n","                if possible_matches_by_order[i] > 0:\n","                    precisions[i] = (float(matches_by_order[i]) /\n","                             possible_matches_by_order[i])\n","                else:\n","                    precisions[i] = 0.0\n","\n","        if min(precisions) > 0:\n","            p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n","            geo_mean = math.exp(p_log_sum)\n","        else:\n","            geo_mean = 0\n","\n","        ratio = float(translation_length) / reference_length\n","\n","        if ratio > 1.0:\n","            bp = 1.\n","        else:\n","            bp = math.exp(1 - 1. / ratio)\n","\n","        bleu = geo_mean * bp\n","\n","        return (bleu, precisions, bp, ratio, translation_length, reference_length)\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:09:02.753730Z","iopub.status.busy":"2024-12-04T05:09:02.753464Z","iopub.status.idle":"2024-12-04T05:09:02.769137Z","shell.execute_reply":"2024-12-04T05:09:02.768494Z","shell.execute_reply.started":"2024-12-04T05:09:02.753710Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n","\n","class BLEUMetric(object):\n","    \n","    def __init__(self, tokenizer, name='bleu_metric', **kwargs):\n","        \"\"\" Computes the BLEU score (Metric for machine translation) \"\"\"\n","        super().__init__()\n","        self.tokenizer = tokenizer\n","    \n","      #self.vocab = vocabulary\n","      #self.id_to_token_layer = StringLookup(vocabulary=self.vocab, num_oov_indices=0, oov_token='[UNKUNK]', invert=True)\n","    \n","    def calculate_bleu_from_predictions(self, real, pred):\n","        \"\"\" Calculate the BLEU score for targets and predictions \"\"\"\n","        \n","        # Get the predicted token IDs\n","        pred_argmax = tf.argmax(pred, axis=-1)  \n","        \n","        # Convert token IDs to words using the vocabulary and the StringLookup\n","        pred_tokens = np.array([[self.tokenizer.id_to_token(pp) for pp in p] for p in pred_argmax])\n","        real_tokens = tf.constant([[self.tokenizer.id_to_token(rr) for rr in r] for r in real])\n","        \n","        def clean_text(tokens):\n","            \n","            \"\"\" Clean padding and other tokens to only keep meaningful words \"\"\"\n","            \n","            # 3. Strip the string of any extra white spaces\n","            translations_in_bytes = tf.strings.strip(\n","                        # 2. Replace everything after the eos token with blank\n","                        tf.strings.regex_replace(\n","                            # 1. Join all the tokens to one string in each sequence\n","                            tf.strings.join(\n","                                tf.transpose(tokens), separator=' '\n","                            ),\n","                        \"\\[END\\].*\", \"\"),\n","                   )\n","            \n","            # Decode the byte stream to a string\n","            translations = np.char.decode( #\n","                translations_in_bytes.numpy().astype(np.bytes_), encoding='utf-8'\n","            )\n","            \n","            # If the string is empty, add a [UNK] token\n","            # Otherwise get a Division by zero error\n","            translations = [sent if len(sent)>0 else \"[UNK]\" for sent in translations ]\n","            \n","            # Split the sequences to individual tokens \n","            translations = np.char.split(translations).tolist()\n","            \n","            return translations\n","        \n","        # Get the clean versions of the predictions and real seuqences\n","        pred_tokens = clean_text(pred_tokens)\n","        # We have to wrap each real sequence in a list to make use of a function to compute bleu\n","        real_tokens = [[token_seq] for token_seq in clean_text(real_tokens)]\n","\n","        # The compute_bleu method accpets the translations and references in the following format\n","        # tranlation - list of list of tokens\n","        # references - list of list of list of tokens\n","        bleu, precisions, bp, ratio, translation_length, reference_length = compute_bleu(real_tokens, pred_tokens, smooth=False)\n","\n","        return bleu"]},{"cell_type":"markdown","metadata":{},"source":["# Training the model"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-12-04T05:09:02.770734Z","iopub.status.busy":"2024-12-04T05:09:02.770516Z","iopub.status.idle":"2024-12-04T08:32:06.713138Z","shell.execute_reply":"2024-12-04T08:32:06.712225Z","shell.execute_reply.started":"2024-12-04T05:09:02.770715Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","Epoch: 1\n","2071/2071 [==============================] - 1692s 721ms/step - loss: 1.3517 - accuracy: 0.7608\n","173 batches processed\n","valid_loss: 1.134343938675919 - valid_accuracy: 0.7831203045183524 - valid_bleu: 0.022141311889752237\n","Epoch: 2\n","2071/2071 [==============================] - 1476s 711ms/step - loss: 1.0708 - accuracy: 0.7899\n","173 batches processed\n","valid_loss: 1.0973282559758666 - valid_accuracy: 0.7856990946510624 - valid_bleu: 0.050972181452681106\n","Epoch: 3\n","2071/2071 [==============================] - 1472s 710ms/step - loss: 1.0348 - accuracy: 0.7936\n","173 batches processed\n","valid_loss: 1.0550943178937615 - valid_accuracy: 0.7914763926770646 - valid_bleu: 0.05261509712741182\n","Epoch: 4\n","2071/2071 [==============================] - 1471s 709ms/step - loss: 1.0270 - accuracy: 0.7938\n","173 batches processed\n","valid_loss: 1.0582506563622138 - valid_accuracy: 0.791956593535539 - valid_bleu: 0.05112908867896077\n","Epoch: 5\n","2071/2071 [==============================] - 1470s 709ms/step - loss: 1.0178 - accuracy: 0.7946\n","173 batches processed\n","valid_loss: 1.0655079246256394 - valid_accuracy: 0.7903843001823205 - valid_bleu: 0.04582702894544065\n"]}],"source":["batch_size=96\n","\n","train_fraction = 0.6\n","valid_fraction = 0.2\n","\n","tokenizer = generate_tokenizer(\n","    train_captions_df, n_vocab=n_vocab\n",")\n","\n","bleu_metric = BLEUMetric(tokenizer=tokenizer)\n","\n","sampled_validation_captions_df = valid_captions_df.sample(frac=valid_fraction)\n","\n","for e in range(5):\n","    print(f\"Epoch: {e+1}\")\n","    \n","    train_dataset, _ = generate_tf_dataset(\n","        train_captions_df.sample(frac=train_fraction), tokenizer=tokenizer, n_vocab=n_vocab, batch_size=batch_size, training=True\n","    )\n","    valid_dataset, _ = generate_tf_dataset(\n","        sampled_validation_captions_df, tokenizer=tokenizer, n_vocab=n_vocab, batch_size=batch_size, training=False\n","    )\n","    \n","    full_model.fit(\n","        train_dataset,\n","        epochs=1\n","    )\n","    \n","    valid_loss, valid_accuracy, valid_bleu = [], [], []\n","    for vi, v_batch in enumerate(valid_dataset):\n","        print(f\"{vi+1} batches processed\", end='\\r')\n","        loss, accuracy = full_model.test_on_batch(v_batch[0], v_batch[1])\n","        batch_predicted = full_model(v_batch[0])\n","        bleu_score = bleu_metric.calculate_bleu_from_predictions(v_batch[1], batch_predicted)\n","        valid_loss.append(loss)\n","        valid_accuracy.append(accuracy)\n","        valid_bleu.append(bleu_score)\n","        \n","    print(\n","        f\"\\nvalid_loss: {np.mean(valid_loss)} - valid_accuracy: {np.mean(valid_accuracy)} - valid_bleu: {np.mean(valid_bleu)}\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":3445072,"sourceId":6019472,"sourceType":"datasetVersion"}],"dockerImageVersionId":30512,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
