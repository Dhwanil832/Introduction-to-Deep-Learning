{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ef8a1e20d7a944fdb6c6d9dd9c322115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9a0434ec08d4e65883201762fb2e89e",
              "IPY_MODEL_8ded144943e04d129401fb46dab58209",
              "IPY_MODEL_5c77232f679e4cd88eaa6dc543a7092b"
            ],
            "layout": "IPY_MODEL_82f9941dbd00475fa171a1439a9847d3"
          }
        },
        "c9a0434ec08d4e65883201762fb2e89e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86cc3f08948f48afbdeede9e932bdf3c",
            "placeholder": "​",
            "style": "IPY_MODEL_2f0d091da4684e0b9c6fe422114749c1",
            "value": "100%"
          }
        },
        "8ded144943e04d129401fb46dab58209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_838cf0a37ebb49ab8c2ee2a1c13e7ebc",
            "max": 35345757,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4938db43477c44c4bab8e95255191fec",
            "value": 35345757
          }
        },
        "5c77232f679e4cd88eaa6dc543a7092b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99a76994e7b6478dadbe832e568bf0f6",
            "placeholder": "​",
            "style": "IPY_MODEL_edfd8faece5644f7bd97d3ed9df9f62b",
            "value": " 33.7M/33.7M [00:00&lt;00:00, 42.5MB/s]"
          }
        },
        "82f9941dbd00475fa171a1439a9847d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86cc3f08948f48afbdeede9e932bdf3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f0d091da4684e0b9c6fe422114749c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "838cf0a37ebb49ab8c2ee2a1c13e7ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4938db43477c44c4bab8e95255191fec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "#a351fb",
            "description_width": ""
          }
        },
        "99a76994e7b6478dadbe832e568bf0f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edfd8faece5644f7bd97d3ed9df9f62b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8qJotnY_ZPZ",
        "outputId": "bfe6f2da-70c3-47df-c179-7cf3c082313d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct  8 20:11:33 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd # /content/sample_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzZYRT8Pxlrh",
        "outputId": "ad705d3e-91b8-4700-bdef-97358359e105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q supervision ultralytics"
      ],
      "metadata": {
        "id": "PZ1wiIW7_g0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "808bcbe5-2214-4cea-8f2c-ddfb1fd42598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/158.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/882.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m882.5/882.5 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "from supervision.assets import VideoAssets, download_assets\n",
        "from collections import defaultdict, deque"
      ],
      "metadata": {
        "id": "_UBPjlug_jfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_assets(VideoAssets.VEHICLES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "ef8a1e20d7a944fdb6c6d9dd9c322115",
            "c9a0434ec08d4e65883201762fb2e89e",
            "8ded144943e04d129401fb46dab58209",
            "5c77232f679e4cd88eaa6dc543a7092b",
            "82f9941dbd00475fa171a1439a9847d3",
            "86cc3f08948f48afbdeede9e932bdf3c",
            "2f0d091da4684e0b9c6fe422114749c1",
            "838cf0a37ebb49ab8c2ee2a1c13e7ebc",
            "4938db43477c44c4bab8e95255191fec",
            "99a76994e7b6478dadbe832e568bf0f6",
            "edfd8faece5644f7bd97d3ed9df9f62b"
          ]
        },
        "id": "se8tTTXx_pfi",
        "outputId": "369cb09e-88b9-4df2-ec49-3af0e7e199cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading vehicles.mp4 assets \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/35345757 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef8a1e20d7a944fdb6c6d9dd9c322115"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'vehicles.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_VIDEO_PATH = \"vehicles.mp4\"\n",
        "TARGET_VIDEO_PATH = \"vehicles-result.mp4\"\n",
        "CONFIDENCE_THRESHOLD = 0.3\n",
        "IOU_THRESHOLD = 0.5\n",
        "MODEL_NAME = \"yolov8x.pt\"\n",
        "MODEL_RESOLUTION = 1280"
      ],
      "metadata": {
        "id": "P6gy3npA_tjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE = np.array([\n",
        "    [1252, 787],\n",
        "    [2298, 803],\n",
        "    [5039, 2159],\n",
        "    [-550, 2159]\n",
        "])\n",
        "\n",
        "TARGET_WIDTH = 25\n",
        "TARGET_HEIGHT = 250\n",
        "\n",
        "TARGET = np.array([\n",
        "    [0, 0],\n",
        "    [TARGET_WIDTH - 1, 0],\n",
        "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
        "    [0, TARGET_HEIGHT - 1],\n",
        "])"
      ],
      "metadata": {
        "id": "teJ-dMYX_vn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "from supervision.assets import VideoAssets, download_assets\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "# Verify if the file exists and print its details\n",
        "SOURCE_VIDEO_PATH = \"vehicles.mp4\"\n",
        "if os.path.exists(SOURCE_VIDEO_PATH):\n",
        "    print(f\"File exists at: {SOURCE_VIDEO_PATH}\")\n",
        "    print(f\"File size: {os.path.getsize(SOURCE_VIDEO_PATH)} bytes\")\n",
        "else:\n",
        "    print(f\"File not found at: {SOURCE_VIDEO_PATH}\")\n",
        "    # Optionally, handle the missing file:\n",
        "    raise FileNotFoundError(f\"Video file not found at {SOURCE_VIDEO_PATH}\")\n",
        "\n",
        "# Try opening the video with cv2.VideoCapture\n",
        "try:\n",
        "    cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)\n",
        "    if not cap.isOpened():\n",
        "        raise IOError  # Raise an exception if it fails\n",
        "    print(\"Video opened successfully.\")\n",
        "    cap.release()  # Release the video capture object\n",
        "except IOError:\n",
        "    print(f\"Error: Could not open video at {SOURCE_VIDEO_PATH}\")\n",
        "    print(\"Possible causes:\")\n",
        "    print(\"- Incorrect file path\")\n",
        "    print(\"- File permissions\")\n",
        "    print(\"- Unsupported video format/codec\")\n",
        "    # Consider alternative solutions or error handling\n",
        "\n",
        "# Custom video frame generator\n",
        "def get_video_frames_generator(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {video_path}\")\n",
        "        return\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break  # End of video\n",
        "        yield frame\n",
        "    cap.release()\n",
        "\n",
        "# Use the custom generator\n",
        "frame_generator = get_video_frames_generator(SOURCE_VIDEO_PATH)\n",
        "frame_iterator = iter(frame_generator)\n",
        "\n",
        "try:\n",
        "    frame = next(frame_iterator)  # Try to get the first frame\n",
        "    print(\"First frame obtained successfully.\")\n",
        "    print(f\"Frame shape: {frame.shape}\")  # Debug frame shape\n",
        "except StopIteration:\n",
        "    print(\"No frames available from the video.\")\n"
      ],
      "metadata": {
        "id": "KT90gHgW_7SU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21048fb8-2ad3-45f0-8f60-04d405c6ed7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File exists at: vehicles.mp4\n",
            "File size: 35345757 bytes\n",
            "Video opened successfully.\n",
            "First frame obtained successfully.\n",
            "Frame shape: (2160, 3840, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ViewTransformer:\n",
        "\n",
        "    def __init__(self, source: np.ndarray, target: np.ndarray) -> None:\n",
        "        source = source.astype(np.float32)\n",
        "        target = target.astype(np.float32)\n",
        "        self.m = cv2.getPerspectiveTransform(source, target)\n",
        "\n",
        "    def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
        "        if points.size == 0:\n",
        "            return points\n",
        "\n",
        "        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
        "        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)\n",
        "        return transformed_points.reshape(-1, 2)"
      ],
      "metadata": {
        "id": "QRyuHh-SCKzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "view_transformer = ViewTransformer(source=SOURCE, target=TARGET)"
      ],
      "metadata": {
        "id": "5wVZ2nA8CODP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade supervision\n",
        "# make sure to restart the runtime to use the updated package\n",
        "\n",
        "import supervision as sv\n",
        "model = YOLO(MODEL_NAME)\n",
        "\n",
        "video_info = sv.VideoInfo.from_video_path(video_path=SOURCE_VIDEO_PATH)\n",
        "frame_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH)\n",
        "\n",
        "# tracer initiation\n",
        "byte_track = sv.ByteTrack(\n",
        "    frame_rate=video_info.fps, track_activation_threshold=CONFIDENCE_THRESHOLD\n",
        ")\n",
        "\n",
        "# annotators configuration\n",
        "thickness = sv.draw.utils.calculate_optimal_line_thickness( # Changed function name to calculate_optimal_line_thickness\n",
        "    resolution_wh=video_info.resolution_wh\n",
        ")\n",
        "text_scale = sv.draw.utils.calculate_optimal_text_scale( # Use calculate_optimal_text_scale\n",
        "    resolution_wh=video_info.resolution_wh\n",
        ")\n",
        "bounding_box_annotator = sv.BoundingBoxAnnotator(\n",
        "    thickness=thickness\n",
        ")\n",
        "label_annotator = sv.LabelAnnotator(\n",
        "    text_scale=text_scale,\n",
        "    text_thickness=thickness,\n",
        "    text_position=sv.Position.BOTTOM_CENTER\n",
        ")\n",
        "trace_annotator = sv.TraceAnnotator(\n",
        "    thickness=thickness,\n",
        "    trace_length=video_info.fps * 2,\n",
        "    position=sv.Position.BOTTOM_CENTER\n",
        ")\n",
        "\n",
        "polygon_zone = sv.PolygonZone(\n",
        "    polygon=SOURCE,\n",
        "    #frame_resolution_wh=video_info.resolution_wh\n",
        ")\n",
        "\n",
        "coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))\n",
        "\n",
        "# open target video\n",
        "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "\n",
        "    # loop over source video frame\n",
        "    for frame in tqdm(frame_generator, total=video_info.total_frames):\n",
        "\n",
        "        result = model(frame, imgsz=MODEL_RESOLUTION, verbose=False)[0]\n",
        "        detections = sv.Detections.from_ultralytics(result)\n",
        "\n",
        "        # filter out detections by class and confidence\n",
        "        detections = detections[detections.confidence > CONFIDENCE_THRESHOLD]\n",
        "        detections = detections[detections.class_id != 0]\n",
        "\n",
        "        # filter out detections outside the zone\n",
        "        detections = detections[polygon_zone.trigger(detections)]\n",
        "\n",
        "        # refine detections using non-max suppression\n",
        "        detections = detections.with_nms(IOU_THRESHOLD)\n",
        "\n",
        "        # pass detection through the tracker\n",
        "        detections = byte_track.update_with_detections(detections=detections)\n",
        "\n",
        "        points = detections.get_anchors_coordinates(\n",
        "            anchor=sv.Position.BOTTOM_CENTER\n",
        "        )\n",
        "\n",
        "        # calculate the detections position inside the target RoI\n",
        "        points = view_transformer.transform_points(points=points).astype(int)\n",
        "\n",
        "        # store detections position\n",
        "        for tracker_id, [_, y] in zip(detections.tracker_id, points):\n",
        "            coordinates[tracker_id].append(y)\n",
        "\n",
        "        # format labels\n",
        "        labels = []\n",
        "\n",
        "        for tracker_id in detections.tracker_id:\n",
        "            if len(coordinates[tracker_id]) < video_info.fps / 2:\n",
        "                labels.append(f\"#{tracker_id}\")\n",
        "            else:\n",
        "                # calculate speed\n",
        "                coordinate_start = coordinates[tracker_id][-1]\n",
        "                coordinate_end = coordinates[tracker_id][0]\n",
        "                distance = abs(coordinate_start - coordinate_end)\n",
        "                time = len(coordinates[tracker_id]) / video_info.fps\n",
        "                speed = distance / time * 3.6\n",
        "                labels.append(f\"#{tracker_id} {int(speed)} km/h\")\n",
        "\n",
        "        # annotate frame\n",
        "        annotated_frame = frame.copy()\n",
        "        annotated_frame = trace_annotator.annotate(\n",
        "            scene=annotated_frame, detections=detections\n",
        "        )\n",
        "        annotated_frame = bounding_box_annotator.annotate(\n",
        "            scene=annotated_frame, detections=detections\n",
        "        )\n",
        "\n",
        "        annotated_frame = label_annotator.annotate(\n",
        "            scene=annotated_frame, detections=detections, labels=labels\n",
        "        )\n",
        "\n",
        "        # add frame\n",
        "        # add frame to target video\n",
        "        sink.write_frame(annotated_frame)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2rztnt7CQsr",
        "outputId": "f471a21b-9ddc-4b91-ba1c-cb694db2971d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: supervision in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from supervision) (0.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from supervision) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.23.3 in /usr/local/lib/python3.10/dist-packages (from supervision) (1.26.4)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.10/dist-packages (from supervision) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=9.4 in /usr/local/lib/python3.10/dist-packages (from supervision) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from supervision) (6.0.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from supervision) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SupervisionWarnings: BoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n",
            "100%|██████████| 538/538 [02:08<00:00,  4.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ffmpeg\n",
        "\n",
        "def convert_mp4_to_mkv(input_file, output_file):\n",
        "    try:\n",
        "        ffmpeg.input(input_file).output(output_file).run()\n",
        "        print(f\"Successfully converted {input_file} to {output_file}\")\n",
        "    except ffmpeg.Error as e:\n",
        "        print(f\"Error converting {input_file} to {output_file}: {e.stderr}\")\n",
        "\n",
        "# Example usage:\n",
        "input_file = '/path/to/input_video.mp4'  # Replace with your input MP4 file path\n",
        "output_file = 'try 1.mkv'  # Replace with your desired output MKV file path\n",
        "\n",
        "convert_mp4_to_mkv(input_file, output_file)\n"
      ],
      "metadata": {
        "id": "wVMtSbRgAII1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the pre-trained YOLOv8 model\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# Define car color labels\n",
        "CAR_COLORS = ['black', 'green', 'yellow', 'red', 'white', 'blue']\n",
        "\n",
        "# Load video file\n",
        "video_path = 'try 1.mkv'\n",
        "#video_path = './000.mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video width, height, and frame rate\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Output video save settings\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter('./try 02.mp4', fourcc, fps, (width, height))\n",
        "\n",
        "\n",
        "def detect_color(image):\n",
        "    # Convert image from BGR to HSV\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Define color ranges (set based on empirical values)\n",
        "    color_ranges = {\n",
        "        'black': [(0, 0, 0), (180, 255, 50)],\n",
        "        'green': [(30, 40, 40), (90, 255, 255)],\n",
        "        'yellow': [(20, 40, 40), (40, 255, 255)],\n",
        "        'red': [(0, 70, 50), (10, 255, 255)],\n",
        "        'white': [(0, 0, 220), (180, 30, 255)],\n",
        "        'blue': [(90, 50, 50), (150, 255, 255)]  # including light and normal blue\n",
        "    }\n",
        "\n",
        "    # Calculate the number of pixels for each color\n",
        "    max_pixels = 0\n",
        "    detected_color = \"black\"\n",
        "    for color, (lower, upper) in color_ranges.items():\n",
        "        mask = cv2.inRange(hsv, np.array(lower), np.array(upper))\n",
        "        count = cv2.countNonZero(mask)\n",
        "        if color == 'red':\n",
        "            # Apply additional condition to differentiate dark red from black\n",
        "            if count > max_pixels and np.mean(hsv[:, :, 2]) > 50:  # Ensure brightness is above a threshold\n",
        "                max_pixels = count\n",
        "                detected_color = color\n",
        "        elif count > max_pixels:\n",
        "            max_pixels = count\n",
        "            detected_color = color\n",
        "\n",
        "    return detected_color\n",
        "\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Use YOLOv8 to detect cars\n",
        "    results = model(frame)\n",
        "    for result in results:\n",
        "        for box in result.boxes:\n",
        "            # Get bounding box coordinates and confidence\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            conf = box.conf[0]\n",
        "            cls = box.cls[0]\n",
        "\n",
        "            # Detect all vehicle types (e.g., car, truck, bus, motorcycle)\n",
        "            if cls in [2, 3, 5, 7]:  # 2: car, 3: motorcycle, 5: bus, 7: truck (using COCO dataset class index)\n",
        "                car_image = frame[y1:y2, x1:x2]\n",
        "\n",
        "                # Detect car color\n",
        "                car_color = detect_color(car_image)\n",
        "\n",
        "                # Draw bounding box and label\n",
        "                label = f'Color: {car_color}'\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    # Write to output video\n",
        "    out.write(frame)\n",
        "\n",
        "    # Display video frame\n",
        "    cv2.imshow('Car Detection', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release resources\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "8DjwWPRDABk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ffmpeg\n",
        "\n",
        "def convert_mp4_to_mkv(input_file, output_file):\n",
        "    try:\n",
        "        ffmpeg.input(input_file).output(output_file).run()\n",
        "        print(f\"Successfully converted {input_file} to {output_file}\")\n",
        "    except ffmpeg.Error as e:\n",
        "        print(f\"Error converting {input_file} to {output_file}: {e.stderr}\")\n",
        "\n",
        "# Example usage:\n",
        "input_file = '/output_video.mp4'  # Replace with your input MP4 file path\n",
        "output_file = 'try 02.mkv'  # Replace with your desired output MKV file path\n",
        "\n",
        "convert_mp4_to_mkv(input_file, output_file)"
      ],
      "metadata": {
        "id": "U_xFiIQEGiBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "from collections import deque\n",
        "\n",
        "# Load YOLO model\n",
        "model = YOLO('./yolov8x.pt')\n",
        "\n",
        "# Video path\n",
        "#video_path = '../video/try 1.mkv'\n",
        "video_path = 'try 02.mkv'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video's height, width, and fps\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Output path\n",
        "output_path = \"./try 03.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# Detection label\n",
        "vehicle_classes = ['car', 'truck']\n",
        "\n",
        "# Record every car's last center\n",
        "tracked_cars = {}\n",
        "current_frame_center = {}\n",
        "previous_frame_center = {}\n",
        "\n",
        "# line position\n",
        "#line1_y = 550\n",
        "#line2_y = 450\n",
        "line1_y = 1500\n",
        "line2_y = 1400\n",
        "\n",
        "car_in = 0\n",
        "truck_in = 0\n",
        "car_out = 0\n",
        "truck_out = 0\n",
        "\n",
        "n = 1\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Complete\")\n",
        "        break\n",
        "\n",
        "    # Use model to predict\n",
        "    results = model(frame)\n",
        "\n",
        "    # Draw the line\n",
        "    color_red = (0, 0, 255)\n",
        "    color_blue = (255, 0, 0)\n",
        "    thickness = 2\n",
        "\n",
        "    cv2.line(frame, (0, line1_y), (frame_width, line1_y), color_red, thickness)\n",
        "    cv2.line(frame, (0, line2_y), (frame_width, line2_y), color_blue, thickness)\n",
        "\n",
        "    # Draw the frame and center point\n",
        "    for result in results[0].boxes:\n",
        "        # class id\n",
        "        class_id = int(result.cls)\n",
        "        # confidence\n",
        "        confidence = result.conf.item()\n",
        "        # class_name\n",
        "        class_name = model.names[class_id]\n",
        "\n",
        "        if n == 1:\n",
        "            if confidence > 0.3 and class_name in vehicle_classes:\n",
        "                x1, y1, x2, y2 = map(int, result.xyxy[0].cpu().numpy())\n",
        "\n",
        "                # center point\n",
        "                center_x = int((x1 + x2) / 2)\n",
        "                center_y = int((y1 + y2) / 2)\n",
        "\n",
        "                # Draw the boundary\n",
        "                label = f\"{class_name} {confidence:.2f}\"\n",
        "                color = (0, 0, 0)\n",
        "\n",
        "                if class_name == \"truck\":\n",
        "                    color = (255, 0, 255)\n",
        "                else:\n",
        "                    color = (0, 255, 0)\n",
        "\n",
        "                #cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "                # Draw the center point\n",
        "                cv2.circle(frame, (center_x, center_y), 5, (0, 0, 255), -1)\n",
        "\n",
        "                # Generate a unique ID\n",
        "                vehicle_id = f\"{class_name}_{center_x}\"\n",
        "                tracked_cars[vehicle_id] = {center_x, center_y}\n",
        "\n",
        "                n += 1\n",
        "        else:\n",
        "            if confidence > 0.3 and class_name in vehicle_classes:\n",
        "                # Get the boundary position\n",
        "                x1, y1, x2, y2 = map(int, result.xyxy[0].cpu().numpy())\n",
        "\n",
        "                # Calculate the center point\n",
        "                center_x = int((x1 + x2) / 2)\n",
        "                center_y = int((y1 + y2) / 2)\n",
        "                #print(\"c1\", center_x, center_y)\n",
        "\n",
        "                previous_center_x, previous_center_y = tracked_cars[vehicle_id]\n",
        "\n",
        "                label = f\"{class_name} {confidence:.2f}\"\n",
        "                color = (0, 0, 0)\n",
        "\n",
        "                if class_name == \"truck\":\n",
        "                    color = (255, 0, 255)\n",
        "                else:\n",
        "                    color = (0, 255, 0)\n",
        "\n",
        "                if vehicle_id in tracked_cars:\n",
        "                    arrow_start = (center_x, center_y)\n",
        "                    arrow_length = 20\n",
        "                    prev_center_x, prev_center_y = tracked_cars[vehicle_id]\n",
        "                    #print(\"c2\", (prev_center_x, prev_center_y))\n",
        "\n",
        "                    # Determine the direction\n",
        "                    if center_x > prev_center_x and center_y < prev_center_y:  # Moving forward\n",
        "                        direction = \"Forward\"\n",
        "                        arrow_end = (center_x + arrow_length, center_y - arrow_length)\n",
        "                    elif center_x < prev_center_x and center_y > prev_center_y:  # Moving backward\n",
        "                        direction = \"Backward\"\n",
        "                        arrow_end = (center_x - arrow_length, center_y + arrow_length)\n",
        "                    else:\n",
        "                        direction = \"Stationary\"\n",
        "                        arrow_end = arrow_start\n",
        "\n",
        "                    # line1_y = 550\n",
        "                    # line2_y = 450\n",
        "                    if center_y - line1_y >= -10 and center_y in range(line2_y, line1_y):\n",
        "                        if class_name == \"truck\":\n",
        "                            truck_in += 1\n",
        "                        elif class_name == \"car\":\n",
        "                            car_in += 1\n",
        "                    elif center_y - line2_y <= 5 and center_y in range(line2_y, line1_y):\n",
        "                        if class_name == \"truck\":\n",
        "                            truck_out += 1\n",
        "                        elif class_name == \"car\":\n",
        "                            car_out += 1\n",
        "\n",
        "                    #cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "                    cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "                    # Draw the center point\n",
        "                    cv2.circle(frame, (center_x, center_y), 5, (0, 0, 255), -1)\n",
        "                    # Draw arrow\n",
        "                    cv2.arrowedLine(frame, arrow_start, arrow_end, color, 2)\n",
        "                    # Draw the direction\n",
        "                    cv2.putText(frame, direction, (center_x, center_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9,\n",
        "                                (255, 255, 255), 2)\n",
        "\n",
        "                    cv2.putText(frame, f\"car_in: {car_in}\", (frame_width - 800, 60), cv2.FONT_HERSHEY_SIMPLEX, 3,\n",
        "                                (255, 255, 255), 2)\n",
        "                    cv2.putText(frame, f\"truck_in: {truck_in}\", (frame_width - 800, 140), cv2.FONT_HERSHEY_SIMPLEX, 3,\n",
        "                                (255, 255, 255), 2)\n",
        "                    cv2.putText(frame, f\"car_out: {car_out}\", (frame_width - 800, 220), cv2.FONT_HERSHEY_SIMPLEX, 3,\n",
        "                                (255, 255, 255), 2)\n",
        "                    cv2.putText(frame, f\"truck_out: {truck_out}\", (frame_width - 800, 300), cv2.FONT_HERSHEY_SIMPLEX, 3,\n",
        "                                (255, 255, 255), 2)\n",
        "\n",
        "                    # Update tracked_cars with the current position\n",
        "                tracked_cars[vehicle_id] = (center_x, center_y)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "    cv2.imshow('frame', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "nFaD2YvDGylA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}